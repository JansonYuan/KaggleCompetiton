{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cd3af8-67a5-4b78-ba3c-d8adb59ca3aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    PreTrainedTokenizerBase,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from peft import PeftModel\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import pandas as pd\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = \"ouput_gemma_9b\"\n",
    "    checkpoint: str = \"/nfs/share/gemma-2-9b-it\"#gemma-2-9b-it\"#gemma-2-9b-it\"  # 4-bit quantized gemma-2-9b-instruct\n",
    "    max_length: int = 3000\n",
    "    n_splits: int = 2\n",
    "    fold_idx: int = 100\n",
    "    optim_type: str = \"paged_adamw_8bit\"\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4  # global batch size is 8\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 1\n",
    "    freeze_layers: int = 0  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 2e-4\n",
    "    warmup_steps: int = 0\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: float = 4\n",
    "    lora_dropout: float = 0.0\n",
    "    lora_bias: str = \"none\"\n",
    "    max_grad_norm: float = 0.0\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    weight_decay: float = 0.01\n",
    "    #fp16: bool = True\n",
    "\n",
    "config = Config()\n",
    "exp_name = f'model_{config.output_dir}_max_length_{config.max_length}_batch_size_{config.per_device_train_batch_size}_accumulation_steps_{config.gradient_accumulation_steps}_lora_r_{config.lora_r}_lora_alpha_{config.lora_alpha}'\n",
    "training_args = TrainingArguments(\n",
    "            output_dir=f\"{config.output_dir}_{config.fold_idx}\",\n",
    "            overwrite_output_dir=True,\n",
    "            gradient_checkpointing=True,\n",
    "            save_total_limit=1,\n",
    "            num_train_epochs=config.n_epochs,\n",
    "            per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "            per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "            logging_steps=2,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            optim=config.optim_type,\n",
    "            lr_scheduler_type=config.lr_scheduler_type,\n",
    "            #bf16=config.fp16,\n",
    "            learning_rate=config.lr,\n",
    "            report_to='none',  # 启用 wandb 日志记录\n",
    "            run_name=exp_name,\n",
    "            warmup_steps=config.warmup_steps,\n",
    "           # deepspeed = 'zero_stage2_config.json' ,\n",
    "            max_grad_norm=config.max_grad_norm,\n",
    "            weight_decay=config.weight_decay,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='acc',\n",
    "            greater_is_better=True,\n",
    "            )\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "\n",
    "\n",
    "ds = Dataset.from_parquet('hf-open-models-v1.parquet')\n",
    "\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer: PreTrainedTokenizerBase,\n",
    "            max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        prompt = [\"<prompt>: \" + t for t in batch[\"prompt\"]]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + t for t in batch[\"response_a\"]]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + t for t in batch[\"response_b\"]]\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "        #labels = [0 if i == 'model_a' else 1 for i in batch[\"winner\"]]\n",
    "        return {**tokenized}\n",
    "\n",
    "\n",
    "\n",
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds = ds.map(encode, batched=True)#.select([i for i in range(500)])\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "    return {\"acc\": acc, \"log_loss\": loss}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f699064c-e6e3-46b2-8ecd-34a063ea452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('train.parquet')#.head(100)\n",
    "# sgkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# folds = []\n",
    "# for fold, (train_idx, test_idx) in enumerate(sgkf.split(df, df['language'])):\n",
    "#     folds.append((train_idx, test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc8de9e-89e5-4409-8c2e-7e950829c7cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fd4384f8564c2cbbeaf5247090a721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /nfs/share/gemma-2-9b-it and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-27 17:08:08,815] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/guanghan/miniconda3/envs/kaggle/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data/guanghan/miniconda3/envs/kaggle/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:435: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    load_in_4bit=True,\n",
    "    num_labels=2,\n",
    "#    torch_dtype=torch.bfloat16,\n",
    "#    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "lora_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        # only target self-attention\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"down_proj\",\"up_proj\",\"o_proj\",\"gate_proj\"],\n",
    "        #layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=config.lora_bias,\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=True,\n",
    "        use_rslora = True,\n",
    "        modules_to_save=[\n",
    "                'score', 'lstm',\n",
    "            ],\n",
    "    )\n",
    "model = PeftModel.from_pretrained(model, f'./ouput_gemma_9b_0', config=lora_config).eval()\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    #train_dataset=ds.select(train_idx),\n",
    "    #eval_dataset=ds.select(eval_idx),\n",
    "    #compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "predictions = trainer.predict(ds).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379115e9-17dc-4480-b29a-9062db12e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_reverse = Dataset.from_parquet('hf-open-models-v1.parquet')\n",
    "\n",
    "\n",
    "class RCustomTokenizer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer: PreTrainedTokenizerBase,\n",
    "            max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        prompt = [\"<prompt>: \" + t for t in batch[\"prompt\"]]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + t for t in batch[\"response_b\"]]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + t for t in batch[\"response_a\"]]\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "        #labels = [0 if i == 'model_a' else 1 for i in batch[\"winner\"]]\n",
    "        return {**tokenized}\n",
    "\n",
    "\n",
    "\n",
    "r_encode = RCustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds_reverse = ds_reverse.map(r_encode, batched=True)#.select([i for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434742ef-c7ec-4304-944b-896648cd7291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_reverse = trainer.predict(ds_reverse).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb900bd-daea-4caa-bd88-3ea381b2cf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2203 , -0.5903 ],\n",
       "       [-1.248  ,  0.3547 ],\n",
       "       [-1.037  , -0.284  ],\n",
       "       ...,\n",
       "       [-0.7236 , -0.48   ],\n",
       "       [-0.927  , -0.05383],\n",
       "       [ 0.5557 , -1.654  ]], dtype=float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171249f6-9962-4edb-a8ac-9b136aea6b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
