{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbeeb999",
   "metadata": {
    "papermill": {
     "duration": 0.003591,
     "end_time": "2024-10-19T10:24:38.980751",
     "exception": false,
     "start_time": "2024-10-19T10:24:38.977160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Based on single LightGBM baseline with minimal FE, LB **0.447**:\n",
    "\n",
    "https://www.kaggle.com/code/snufkin77/mcts-strength-relevant-baseline\n",
    "\n",
    "DeepTables: Deep-learning Toolkit for Tabular data\n",
    "\n",
    "https://github.com/DataCanvasIO/DeepTables\n",
    "\n",
    "https://deeptables.readthedocs.io/en/latest/model_config.html#parameters\n",
    "\n",
    "**Version 1**: single DeepTables NN baseline, LB **0.462**.\n",
    "\n",
    "**Version 6**: single DeepTables NN, LB **0.448**; `ModelConfig(apply_gbm_features=True)`.\n",
    "\n",
    "**Version 7**: single DeepTables NN, LB **0.438**; `ModelConfig(apply_gbm_features=True)`, `ModelConfig(nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'])`.\n",
    "\n",
    "**Version 8**: same as Version 7 + scaling all numerical features to fix issue with divergence in the validation scores. Now overall CV rmse (0.4324) is closer to LB **0.435**. Note that in this version, the `fit_transform` method of scaler was performed on train and test data combined, which is not a good practice, although it does not violate any Kaggle rules.\n",
    "\n",
    "**Version 9**: same as Version 8, but using scaler with `fit_transform` on the training data alone and then applying `transform` to the test data. CV 0.4319 | LB **0.438**.\n",
    "\n",
    "**Version 10**: scaling way from Version 9, enabling `LearningRateScheduler` with warmup (`LR_START = 1e-4`) on first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c990a3e1",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-10-19T10:24:38.988297Z",
     "iopub.status.busy": "2024-10-19T10:24:38.987984Z",
     "iopub.status.idle": "2024-10-19T10:25:54.788365Z",
     "shell.execute_reply": "2024-10-19T10:25:54.787249Z"
    },
    "papermill": {
     "duration": 75.806513,
     "end_time": "2024-10-19T10:25:54.790606",
     "exception": false,
     "start_time": "2024-10-19T10:24:38.984093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/tensorflow-2-15/tensorflow\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.5.4)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.11.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (18.1.1)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.26.4)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (70.0.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (4.12.2)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.37.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.62.2)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/tensorboard-2.15.1-py3-none-any.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.0)\r\n",
      "Processing /kaggle/input/tensorflow-2-15/tensorflow/keras-2.15.0-py3-none-any.whl (from tensorflow==2.15.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.30.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.4)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15.0) (3.1.2)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\r\n",
      "Installing collected packages: wrapt, ml-dtypes, keras, tensorboard, tensorflow\r\n",
      "  Attempting uninstall: wrapt\r\n",
      "    Found existing installation: wrapt 1.16.0\r\n",
      "    Uninstalling wrapt-1.16.0:\r\n",
      "      Successfully uninstalled wrapt-1.16.0\r\n",
      "  Attempting uninstall: ml-dtypes\r\n",
      "    Found existing installation: ml-dtypes 0.3.2\r\n",
      "    Uninstalling ml-dtypes-0.3.2:\r\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.3.3\r\n",
      "    Uninstalling keras-3.3.3:\r\n",
      "      Successfully uninstalled keras-3.3.3\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.16.2\r\n",
      "    Uninstalling tensorboard-2.16.2:\r\n",
      "      Successfully uninstalled tensorboard-2.16.2\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.16.1\r\n",
      "    Uninstalling tensorflow-2.16.1:\r\n",
      "      Successfully uninstalled tensorflow-2.16.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\r\n",
      "tensorstore 0.1.65 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\r\n",
      "tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.1 tensorflow-2.15.0 wrapt-1.14.1\r\n",
      "Looking in links: /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/deeptables-0.2.5-py3-none-any.whl\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (21.3)\r\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (1.14.1)\r\n",
      "Requirement already satisfied: pandas>=0.25.3 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (2.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (1.2.2)\r\n",
      "Requirement already satisfied: lightgbm>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (4.2.0)\r\n",
      "Requirement already satisfied: category-encoders>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (2.6.3)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/hypernets-0.3.1-py3-none-any.whl (from deeptables==0.2.5)\r\n",
      "Requirement already satisfied: h5py>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (3.11.0)\r\n",
      "Requirement already satisfied: eli5 in /opt/conda/lib/python3.10/site-packages (from deeptables==0.2.5) (0.13.0)\r\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.14.2)\r\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from category-encoders>=2.1.0->deeptables==0.2.5) (0.5.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2024.6.1)\r\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (8.21.0)\r\n",
      "Requirement already satisfied: traitlets in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.14.3)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/XlsxWriter-3.1.9-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.0.2)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/paramiko-3.4.0-py3-none-any.whl (from hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (2.32.3)\r\n",
      "Requirement already satisfied: tornado in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (6.4.1)\r\n",
      "Requirement already satisfied: prettytable in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (3.10.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (4.66.4)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from hypernets>=0.2.5.1->deeptables==0.2.5) (1.4.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.3->deeptables==0.2.5) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.3->deeptables==0.2.5) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25.3->deeptables==0.2.5) (2024.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->deeptables==0.2.5) (3.5.0)\r\n",
      "Requirement already satisfied: attrs>17.1.0 in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (23.2.0)\r\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (3.1.4)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (1.16.0)\r\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (0.20.3)\r\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from eli5->deeptables==0.2.5) (0.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->deeptables==0.2.5) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->eli5->deeptables==0.2.5) (2.1.5)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.1.7)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (3.0.47)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.18.0)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.6.2)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (1.2.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (4.9.0)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.10/site-packages (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (42.0.8)\r\n",
      "Processing /kaggle/input/deeptables-v0-2-5/deeptables-0.2.5/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (from paramiko->hypernets>=0.2.5.1->deeptables==0.2.5)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.13)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->hypernets>=0.2.5.1->deeptables==0.2.5) (2024.8.30)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (1.16.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.7.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.0.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->hypernets>=0.2.5.1->deeptables==0.2.5) (0.2.2)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko->hypernets>=0.2.5.1->deeptables==0.2.5) (2.22)\r\n",
      "Installing collected packages: XlsxWriter, bcrypt, pynacl, paramiko, hypernets, deeptables\r\n",
      "Successfully installed XlsxWriter-3.1.9 bcrypt-4.1.2 deeptables-0.2.5 hypernets-0.3.1 paramiko-3.4.0 pynacl-1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-index -U --find-links=/kaggle/input/tensorflow-2-15/tensorflow tensorflow==2.15.0\n",
    "!pip install --no-index -U --find-links=/kaggle/input/deeptables-v0-2-5/deeptables-0.2.5 deeptables==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e46529e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T10:25:54.806660Z",
     "iopub.status.busy": "2024-10-19T10:25:54.805918Z",
     "iopub.status.idle": "2024-10-19T10:26:09.795202Z",
     "shell.execute_reply": "2024-10-19T10:26:09.794279Z"
    },
    "papermill": {
     "duration": 14.999669,
     "end_time": "2024-10-19T10:26:09.797513",
     "exception": false,
     "start_time": "2024-10-19T10:25:54.797844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 10:25:57.425810: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-19 10:25:57.425885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-19 10:25:57.427306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0, GPU = True\n",
      "DeepTables version: 0.2.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from colorama import Fore, Style\n",
    "\n",
    "import tensorflow as tf, deeptables as dt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from deeptables.models import DeepTable, ModelConfig\n",
    "from deeptables.models import deepnets\n",
    "\n",
    "import kaggle_evaluation.mcts_inference_server\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('TensorFlow version:',tf.__version__+',',\n",
    "      'GPU =',tf.test.is_gpu_available())\n",
    "print('DeepTables version:',dt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314a4a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T10:26:09.814075Z",
     "iopub.status.busy": "2024-10-19T10:26:09.813352Z",
     "iopub.status.idle": "2024-10-19T10:26:09.819139Z",
     "shell.execute_reply": "2024-10-19T10:26:09.818289Z"
    },
    "papermill": {
     "duration": 0.015969,
     "end_time": "2024-10-19T10:26:09.821021",
     "exception": false,
     "start_time": "2024-10-19T10:26:09.805052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce30acb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T10:26:09.836568Z",
     "iopub.status.busy": "2024-10-19T10:26:09.836287Z",
     "iopub.status.idle": "2024-10-19T10:26:09.881571Z",
     "shell.execute_reply": "2024-10-19T10:26:09.880902Z"
    },
    "papermill": {
     "duration": 0.05514,
     "end_time": "2024-10-19T10:26:09.883506",
     "exception": false,
     "start_time": "2024-10-19T10:26:09.828366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "constant_cols = pd.read_csv('/kaggle/input/um-gps-of-mcts-variants-constant-columns/constant_columns.csv').columns.to_list()\n",
    "target_col = 'utility_agent1'\n",
    "game_col = 'GameRulesetName'\n",
    "game_rule_cols = ['EnglishRules', 'LudRules']\n",
    "output_cols = ['num_wins_agent1', 'num_draws_agent1', 'num_losses_agent1']\n",
    "dropped_cols = ['Id'] + constant_cols + game_rule_cols + output_cols\n",
    "agent_cols = ['agent1', 'agent2']\n",
    "\n",
    "def preprocess_data(df): \n",
    "    df = df.drop(filter(lambda x: x in df.columns, dropped_cols))\n",
    "    if CFG.split_agent_features:\n",
    "        for col in agent_cols:\n",
    "            df = df.with_columns(pl.col(col).str.split(by=\"-\").list.to_struct(fields=lambda idx: f\"{col}_{idx}\")).unnest(col).drop(f\"{col}_0\")\n",
    "    df = df.with_columns([pl.col(col).cast(pl.Categorical) for col in df.columns if col[:6] in agent_cols])            \n",
    "    df = df.with_columns([pl.col(col).cast(pl.Float32) for col in df.columns if col[:6] not in agent_cols and col != game_col])\n",
    "    df = df.to_pandas()\n",
    "    print(f'Data shape: {df.shape}\\n')\n",
    "    num_cols = df.select_dtypes(exclude=['category']).columns.tolist()\n",
    "    num_cols = [num for num in num_cols if num not in [target_col, game_col]]\n",
    "    return df, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e4586a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T10:26:09.900412Z",
     "iopub.status.busy": "2024-10-19T10:26:09.899726Z",
     "iopub.status.idle": "2024-10-19T10:26:10.196880Z",
     "shell.execute_reply": "2024-10-19T10:26:10.195795Z"
    },
    "papermill": {
     "duration": 0.308478,
     "end_time": "2024-10-19T10:26:10.199156",
     "exception": false,
     "start_time": "2024-10-19T10:26:09.890678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 0.0001 to 0.001 to 0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790/notebook\n",
    "LR_START = 1e-4\n",
    "LR_MAX = 1e-3\n",
    "LR_MIN = 1e-3\n",
    "LR_RAMPUP_EPOCHS = 1\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "EPOCHS = 7\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n",
    "        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n",
    "        phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "        cosine_decay = 0.5 * (1 + math.cos(phase))\n",
    "        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN    \n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "lr_y = [lrfn(x) for x in rng]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rng, lr_y, '-o')\n",
    "plt.xlabel('Epoch'); plt.ylabel('LR')\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n",
    "      format(lr_y[0], max(lr_y), lr_y[-1]))\n",
    "LR_Scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c2c96b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T10:26:10.217207Z",
     "iopub.status.busy": "2024-10-19T10:26:10.216890Z",
     "iopub.status.idle": "2024-10-19T10:26:10.225757Z",
     "shell.execute_reply": "2024-10-19T10:26:10.224892Z"
    },
    "papermill": {
     "duration": 0.01983,
     "end_time": "2024-10-19T10:26:10.227627",
     "exception": false,
     "start_time": "2024-10-19T10:26:10.207797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_path = '/kaggle/input/um-game-playing-strength-of-mcts-variants/train.csv'\n",
    "    split_agent_features = True\n",
    "    scaler = MinMaxScaler()  # Scaler or None\n",
    "    folds = 6\n",
    "    epochs = 7\n",
    "    batch_size = 128\n",
    "    LR_Scheduler = [LR_Scheduler]\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    \n",
    "    conf = ModelConfig(auto_imputation=False,\n",
    "                       auto_discrete=False,\n",
    "                       auto_discard_unique=True,\n",
    "                       categorical_columns='auto',\n",
    "                       apply_gbm_features=True,\n",
    "                       fixed_embedding_dim=True,\n",
    "                       embeddings_output_dim=4,\n",
    "                       embedding_dropout=0.2,\n",
    "                       nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'],\n",
    "                       dnn_params={\n",
    "                           'hidden_units': ((1024, 0.0, True),\n",
    "                                            (512, 0.0, True),\n",
    "                                            (256, 0.0, True),\n",
    "                                            (128, 0.0, True)),\n",
    "                           'dnn_activation': 'relu',\n",
    "                       },\n",
    "                       stacking_op='concat',\n",
    "                       output_use_bias=False,\n",
    "                       optimizer=optimizer,\n",
    "                       task='regression',\n",
    "                       loss='auto',\n",
    "                       metrics=[\"RootMeanSquaredError\"],\n",
    "                       earlystopping_patience=1,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3c8880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-19T10:26:10.245108Z",
     "iopub.status.busy": "2024-10-19T10:26:10.244820Z",
     "iopub.status.idle": "2024-10-19T10:26:10.257411Z",
     "shell.execute_reply": "2024-10-19T10:26:10.256565Z"
    },
    "papermill": {
     "duration": 0.023266,
     "end_time": "2024-10-19T10:26:10.259405",
     "exception": false,
     "start_time": "2024-10-19T10:26:10.236139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_nn(data):\n",
    "    cv = GroupKFold(n_splits=CFG.folds)\n",
    "    groups = data[game_col]\n",
    "    X = data.drop([target_col, game_col], axis=1)\n",
    "    y = data[target_col]\n",
    "    oof = np.zeros(len(data))\n",
    "    models = []\n",
    "    \n",
    "    for fi, (train_idx, valid_idx) in enumerate(cv.split(X, y, groups)):\n",
    "        print(\"#\"*25)\n",
    "        print(f\"### Fold {fi+1}/{CFG.folds} ...\")\n",
    "        print(\"#\"*25)   \n",
    "        K.clear_session()\n",
    "        model = DeepTable(config=CFG.conf)\n",
    "        model.fit(X.iloc[train_idx], y.iloc[train_idx],\n",
    "                  validation_data=(X.iloc[valid_idx], y.iloc[valid_idx]),\n",
    "                  callbacks=CFG.LR_Scheduler,\n",
    "                  batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n",
    "        models.append(model)\n",
    "        \n",
    "        # Avoid some errors\n",
    "        with K.name_scope(CFG.optimizer.__class__.__name__):\n",
    "            for j, var in enumerate(CFG.optimizer.weights):\n",
    "                name = 'variable{}'.format(j)\n",
    "                CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n",
    "        CFG.conf = CFG.conf._replace(optimizer=CFG.optimizer)\n",
    "        \n",
    "        oof_preds = model.predict(X.iloc[valid_idx], verbose=1, batch_size=512).flatten()\n",
    "        rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n",
    "        print(f'{Fore.GREEN}{Style.BRIGHT}\\nFold {fi+1} | rmse: {rmse}\\n')\n",
    "        if fi<CFG.folds: oof[valid_idx] = oof_preds\n",
    "        else: oof[valid_idx] += oof_preds\n",
    "            \n",
    "    rmse = np.round(np.sqrt(np.mean((oof - y)**2)),4)\n",
    "    print(f'{Fore.BLUE}{Style.BRIGHT}Overall CV rmse: {rmse}\\n')\n",
    "    plot_model(model.get_model().model)\n",
    "    return models\n",
    "\n",
    "def infer_nn(data, models):\n",
    "    return np.mean([model.predict(data, verbose=1, batch_size=512).flatten() for model in models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09a2fed",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-10-19T10:26:10.276195Z",
     "iopub.status.busy": "2024-10-19T10:26:10.275930Z",
     "iopub.status.idle": "2024-10-19T10:54:18.148558Z",
     "shell.execute_reply": "2024-10-19T10:54:18.147702Z"
    },
    "papermill": {
     "duration": 1687.883494,
     "end_time": "2024-10-19T10:54:18.150693",
     "exception": false,
     "start_time": "2024-10-19T10:26:10.267199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (233234, 598)\n",
      "\n",
      "Scaling 588 numerical cols.\n",
      "\n",
      "#########################\n",
      "### Fold 1/6 ...\n",
      "#########################\n",
      "10-19 10:26:21 I deeptables.m.deeptable.py 338 - X.Shape=(194328, 596), y.Shape=(194328,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'fm_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7e095da81c30>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "10-19 10:26:21 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-19 10:26:23 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-19 10:26:23 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "10-19 10:26:24 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.6557238101959229s\n",
      "10-19 10:26:24 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "10-19 10:26:24 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.38260579109191895s\n",
      "10-19 10:26:24 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "10-19 10:26:25 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.348956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17137\n",
      "[LightGBM] [Info] Number of data points in the train set: 194328, number of used features: 593\n",
      "[LightGBM] [Info] Start training from score 0.048944\n",
      "10-19 10:26:45 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 20.867204666137695s\n",
      "10-19 10:26:46 I deeptables.m.preprocessor.py 196 - fit_transform taken 23.416436672210693s\n",
      "10-19 10:26:46 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:26:47 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8782908916473389s\n",
      "10-19 10:26:47 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "10-19 10:26:47 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0006716251373291016s\n",
      "10-19 10:26:47 I deeptables.m.deeptable.py 354 - Training...\n",
      "10-19 10:26:47 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:1, mode:min\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "10-19 10:26:47 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:26:51 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:26:52 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "10-19 10:26:53 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (585)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1017)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'fm_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1017), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "10-19 10:26:53 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "1518/1518 - 52s - loss: 0.1403 - root_mean_squared_error: 0.3745 - val_loss: 0.2149 - val_root_mean_squared_error: 0.4636 - lr: 1.0000e-04 - 52s/epoch - 34ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "1518/1518 - 32s - loss: 0.0934 - root_mean_squared_error: 0.3056 - val_loss: 0.1960 - val_root_mean_squared_error: 0.4427 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "1518/1518 - 32s - loss: 0.0751 - root_mean_squared_error: 0.2740 - val_loss: 0.1946 - val_root_mean_squared_error: 0.4411 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "1518/1518 - 32s - loss: 0.0690 - root_mean_squared_error: 0.2627 - val_loss: 0.2000 - val_root_mean_squared_error: 0.4472 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "1518/1518 - 32s - loss: 0.0644 - root_mean_squared_error: 0.2538 - val_loss: 0.1963 - val_root_mean_squared_error: 0.4431 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "1518/1518 - 32s - loss: 0.0608 - root_mean_squared_error: 0.2465 - val_loss: 0.1910 - val_root_mean_squared_error: 0.4371 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "1518/1518 - 32s - loss: 0.0575 - root_mean_squared_error: 0.2397 - val_loss: 0.1936 - val_root_mean_squared_error: 0.4400 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "10-19 10:30:57 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "10-19 10:30:57 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "10-19 10:30:58 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241019102620_cin_nets_fm_nets_dnn_nets/cin_nets+fm_nets+dnn_nets.h5\n",
      "10-19 10:30:58 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:30:58 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:30:59 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8870203495025635s\n",
      "10-19 10:30:59 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:30:59 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "76/76 [==============================] - 3s 19ms/step\n",
      "10-19 10:31:02 I deeptables.m.deeptable.py 559 - predict_proba taken 4.020008563995361s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 1 | rmse: 0.4399999976158142\n",
      "\n",
      "#########################\n",
      "### Fold 2/6 ...\n",
      "#########################\n",
      "10-19 10:31:03 I deeptables.m.deeptable.py 338 - X.Shape=(194394, 596), y.Shape=(194394,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'fm_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7e095da81c30>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "10-19 10:31:03 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-19 10:31:04 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-19 10:31:05 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "10-19 10:31:05 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.6524755954742432s\n",
      "10-19 10:31:05 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "10-19 10:31:06 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.3624684810638428s\n",
      "10-19 10:31:06 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "10-19 10:31:07 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.368980 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17176\n",
      "[LightGBM] [Info] Number of data points in the train set: 194394, number of used features: 593\n",
      "[LightGBM] [Info] Start training from score 0.049932\n",
      "10-19 10:31:26 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 20.591280698776245s\n",
      "10-19 10:31:27 I deeptables.m.preprocessor.py 196 - fit_transform taken 23.105408430099487s\n",
      "10-19 10:31:27 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:31:28 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8846449851989746s\n",
      "10-19 10:31:28 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "10-19 10:31:28 I deeptables.m.preprocessor.py 236 - transform_y taken 0.00034546852111816406s\n",
      "10-19 10:31:29 I deeptables.m.deeptable.py 354 - Training...\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "10-19 10:31:29 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:31:31 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:31:31 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "10-19 10:31:32 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (585)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1017)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'fm_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1017), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "10-19 10:31:32 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "1518/1518 - 48s - loss: 0.1666 - root_mean_squared_error: 0.4082 - val_loss: 0.2388 - val_root_mean_squared_error: 0.4887 - lr: 1.0000e-04 - 48s/epoch - 32ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "1518/1518 - 32s - loss: 0.1029 - root_mean_squared_error: 0.3208 - val_loss: 0.2078 - val_root_mean_squared_error: 0.4559 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "1518/1518 - 32s - loss: 0.0799 - root_mean_squared_error: 0.2827 - val_loss: 0.2087 - val_root_mean_squared_error: 0.4568 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "1518/1518 - 32s - loss: 0.0731 - root_mean_squared_error: 0.2703 - val_loss: 0.2067 - val_root_mean_squared_error: 0.4546 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "1518/1518 - 32s - loss: 0.0679 - root_mean_squared_error: 0.2606 - val_loss: 0.2111 - val_root_mean_squared_error: 0.4594 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "1518/1518 - 32s - loss: 0.0635 - root_mean_squared_error: 0.2521 - val_loss: 0.2081 - val_root_mean_squared_error: 0.4562 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "1518/1518 - 32s - loss: 0.0599 - root_mean_squared_error: 0.2448 - val_loss: 0.2065 - val_root_mean_squared_error: 0.4544 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "10-19 10:35:32 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "10-19 10:35:32 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "10-19 10:35:33 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241019103102_cin_nets_fm_nets_dnn_nets/cin_nets+fm_nets+dnn_nets.h5\n",
      "10-19 10:35:34 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:35:34 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:35:35 I deeptables.m.preprocessor.py 249 - transform_X taken 1.3600802421569824s\n",
      "10-19 10:35:35 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:35:35 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "76/76 [==============================] - 2s 19ms/step\n",
      "10-19 10:35:38 I deeptables.m.deeptable.py 559 - predict_proba taken 3.9534249305725098s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 2 | rmse: 0.4544000029563904\n",
      "\n",
      "#########################\n",
      "### Fold 3/6 ...\n",
      "#########################\n",
      "10-19 10:35:38 I deeptables.m.deeptable.py 338 - X.Shape=(194328, 596), y.Shape=(194328,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'fm_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7e095da81c30>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "10-19 10:35:38 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-19 10:35:40 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-19 10:35:40 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "10-19 10:35:41 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.6427521705627441s\n",
      "10-19 10:35:41 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "10-19 10:35:41 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.36602258682250977s\n",
      "10-19 10:35:41 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "10-19 10:35:42 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.358621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17212\n",
      "[LightGBM] [Info] Number of data points in the train set: 194328, number of used features: 594\n",
      "[LightGBM] [Info] Start training from score 0.044576\n",
      "10-19 10:36:02 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 20.54023289680481s\n",
      "10-19 10:36:03 I deeptables.m.preprocessor.py 196 - fit_transform taken 23.065612077713013s\n",
      "10-19 10:36:03 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:36:04 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8940598964691162s\n",
      "10-19 10:36:04 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "10-19 10:36:04 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0005812644958496094s\n",
      "10-19 10:36:04 I deeptables.m.deeptable.py 354 - Training...\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "10-19 10:36:04 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:36:07 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:36:07 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "10-19 10:36:08 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (586)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1018)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'fm_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1018), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "10-19 10:36:08 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "1518/1518 - 49s - loss: 0.1373 - root_mean_squared_error: 0.3706 - val_loss: 0.1939 - val_root_mean_squared_error: 0.4403 - lr: 1.0000e-04 - 49s/epoch - 32ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "1518/1518 - 32s - loss: 0.0966 - root_mean_squared_error: 0.3108 - val_loss: 0.1708 - val_root_mean_squared_error: 0.4133 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "1518/1518 - 32s - loss: 0.0778 - root_mean_squared_error: 0.2790 - val_loss: 0.1659 - val_root_mean_squared_error: 0.4073 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "1518/1518 - 32s - loss: 0.0715 - root_mean_squared_error: 0.2674 - val_loss: 0.1684 - val_root_mean_squared_error: 0.4103 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "1518/1518 - 32s - loss: 0.0667 - root_mean_squared_error: 0.2583 - val_loss: 0.1634 - val_root_mean_squared_error: 0.4043 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "1518/1518 - 32s - loss: 0.0627 - root_mean_squared_error: 0.2504 - val_loss: 0.1618 - val_root_mean_squared_error: 0.4022 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "1518/1518 - 32s - loss: 0.0592 - root_mean_squared_error: 0.2433 - val_loss: 0.1638 - val_root_mean_squared_error: 0.4047 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "10-19 10:40:08 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "10-19 10:40:08 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "10-19 10:40:09 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241019103538_cin_nets_fm_nets_dnn_nets/cin_nets+fm_nets+dnn_nets.h5\n",
      "10-19 10:40:10 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:40:10 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:40:11 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8913581371307373s\n",
      "10-19 10:40:11 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:40:11 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "76/76 [==============================] - 2s 14ms/step\n",
      "10-19 10:40:13 I deeptables.m.deeptable.py 559 - predict_proba taken 3.146677255630493s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 3 | rmse: 0.40459999442100525\n",
      "\n",
      "#########################\n",
      "### Fold 4/6 ...\n",
      "#########################\n",
      "10-19 10:40:14 I deeptables.m.deeptable.py 338 - X.Shape=(194372, 596), y.Shape=(194372,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'fm_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7e095da81c30>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "10-19 10:40:14 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-19 10:40:16 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-19 10:40:17 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "10-19 10:40:17 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.6498260498046875s\n",
      "10-19 10:40:17 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "10-19 10:40:18 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.3647944927215576s\n",
      "10-19 10:40:18 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "10-19 10:40:18 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.375267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17149\n",
      "[LightGBM] [Info] Number of data points in the train set: 194372, number of used features: 583\n",
      "[LightGBM] [Info] Start training from score 0.038129\n",
      "10-19 10:40:38 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 20.52427053451538s\n",
      "10-19 10:40:39 I deeptables.m.preprocessor.py 196 - fit_transform taken 23.007123231887817s\n",
      "10-19 10:40:39 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:40:40 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8975677490234375s\n",
      "10-19 10:40:40 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "10-19 10:40:40 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0005724430084228516s\n",
      "10-19 10:40:40 I deeptables.m.deeptable.py 354 - Training...\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "10-19 10:40:40 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:40:43 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:40:43 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "10-19 10:40:44 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (575)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1007)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'fm_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1007), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "10-19 10:40:44 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "1518/1518 - 48s - loss: 0.1698 - root_mean_squared_error: 0.4121 - val_loss: 0.2374 - val_root_mean_squared_error: 0.4872 - lr: 1.0000e-04 - 48s/epoch - 32ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "1518/1518 - 32s - loss: 0.1011 - root_mean_squared_error: 0.3179 - val_loss: 0.2029 - val_root_mean_squared_error: 0.4504 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "1518/1518 - 32s - loss: 0.0802 - root_mean_squared_error: 0.2832 - val_loss: 0.1981 - val_root_mean_squared_error: 0.4451 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "1518/1518 - 32s - loss: 0.0745 - root_mean_squared_error: 0.2730 - val_loss: 0.1961 - val_root_mean_squared_error: 0.4429 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "1518/1518 - 32s - loss: 0.0700 - root_mean_squared_error: 0.2645 - val_loss: 0.1988 - val_root_mean_squared_error: 0.4459 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "1518/1518 - 32s - loss: 0.0659 - root_mean_squared_error: 0.2566 - val_loss: 0.1991 - val_root_mean_squared_error: 0.4462 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "1518/1518 - 32s - loss: 0.0622 - root_mean_squared_error: 0.2493 - val_loss: 0.1907 - val_root_mean_squared_error: 0.4367 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "10-19 10:44:46 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "10-19 10:44:46 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "10-19 10:44:47 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241019104014_cin_nets_fm_nets_dnn_nets/cin_nets+fm_nets+dnn_nets.h5\n",
      "10-19 10:44:48 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:44:48 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:44:49 I deeptables.m.preprocessor.py 249 - transform_X taken 1.5862910747528076s\n",
      "10-19 10:44:49 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:44:49 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "76/76 [==============================] - 2s 19ms/step\n",
      "10-19 10:44:52 I deeptables.m.deeptable.py 559 - predict_proba taken 4.209361791610718s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 4 | rmse: 0.4368000030517578\n",
      "\n",
      "#########################\n",
      "### Fold 5/6 ...\n",
      "#########################\n",
      "10-19 10:44:53 I deeptables.m.deeptable.py 338 - X.Shape=(194374, 596), y.Shape=(194374,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'fm_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7e095da81c30>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "10-19 10:44:53 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-19 10:44:55 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-19 10:44:55 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "10-19 10:44:56 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.7187507152557373s\n",
      "10-19 10:44:56 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "10-19 10:44:57 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.46407151222229004s\n",
      "10-19 10:44:57 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "10-19 10:44:57 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.358292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16991\n",
      "[LightGBM] [Info] Number of data points in the train set: 194374, number of used features: 595\n",
      "[LightGBM] [Info] Start training from score 0.031522\n",
      "10-19 10:45:17 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 20.50015354156494s\n",
      "10-19 10:45:18 I deeptables.m.preprocessor.py 196 - fit_transform taken 23.187135219573975s\n",
      "10-19 10:45:18 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:45:19 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8854179382324219s\n",
      "10-19 10:45:19 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "10-19 10:45:19 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0008704662322998047s\n",
      "10-19 10:45:19 I deeptables.m.deeptable.py 354 - Training...\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "10-19 10:45:19 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:45:22 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:45:22 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "10-19 10:45:23 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (587)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1019)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'fm_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1019), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "10-19 10:45:23 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "1518/1518 - 48s - loss: 0.1197 - root_mean_squared_error: 0.3459 - val_loss: 0.2025 - val_root_mean_squared_error: 0.4500 - lr: 1.0000e-04 - 48s/epoch - 32ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "1518/1518 - 32s - loss: 0.0923 - root_mean_squared_error: 0.3038 - val_loss: 0.1954 - val_root_mean_squared_error: 0.4420 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "1518/1518 - 32s - loss: 0.0755 - root_mean_squared_error: 0.2748 - val_loss: 0.1974 - val_root_mean_squared_error: 0.4443 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "1518/1518 - 32s - loss: 0.0691 - root_mean_squared_error: 0.2628 - val_loss: 0.1927 - val_root_mean_squared_error: 0.4390 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "1518/1518 - 32s - loss: 0.0641 - root_mean_squared_error: 0.2532 - val_loss: 0.1937 - val_root_mean_squared_error: 0.4401 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "1518/1518 - 32s - loss: 0.0603 - root_mean_squared_error: 0.2456 - val_loss: 0.1920 - val_root_mean_squared_error: 0.4382 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "1518/1518 - 32s - loss: 0.0568 - root_mean_squared_error: 0.2383 - val_loss: 0.1924 - val_root_mean_squared_error: 0.4387 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "10-19 10:49:24 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "10-19 10:49:24 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "10-19 10:49:25 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241019104452_cin_nets_fm_nets_dnn_nets/cin_nets+fm_nets+dnn_nets.h5\n",
      "10-19 10:49:27 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:49:27 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:49:28 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8791439533233643s\n",
      "10-19 10:49:28 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:49:28 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "76/76 [==============================] - 2s 19ms/step\n",
      "10-19 10:49:30 I deeptables.m.deeptable.py 559 - predict_proba taken 3.4632327556610107s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 5 | rmse: 0.43869999051094055\n",
      "\n",
      "#########################\n",
      "### Fold 6/6 ...\n",
      "#########################\n",
      "10-19 10:49:31 I deeptables.m.deeptable.py 338 - X.Shape=(194374, 596), y.Shape=(194374,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'fm_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x7e095da81c30>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "10-19 10:49:31 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10-19 10:49:34 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-19 10:49:34 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "10-19 10:49:35 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.6511979103088379s\n",
      "10-19 10:49:35 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "10-19 10:49:35 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.43283915519714355s\n",
      "10-19 10:49:35 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "10-19 10:49:36 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.663775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17062\n",
      "[LightGBM] [Info] Number of data points in the train set: 194374, number of used features: 587\n",
      "[LightGBM] [Info] Start training from score 0.053284\n",
      "10-19 10:50:00 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 25.3984317779541s\n",
      "10-19 10:50:02 I deeptables.m.preprocessor.py 196 - fit_transform taken 27.960931062698364s\n",
      "10-19 10:50:02 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:50:02 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8711483478546143s\n",
      "10-19 10:50:02 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "10-19 10:50:02 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0009763240814208984s\n",
      "10-19 10:50:03 I deeptables.m.deeptable.py 354 - Training...\n",
      "2 Physical GPUs, 2 Logical GPUs\n",
      "10-19 10:50:03 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:50:05 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "10-19 10:50:06 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "10-19 10:50:07 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (579)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1011)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'fm_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1011), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "10-19 10:50:07 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "1518/1518 - 48s - loss: 0.1254 - root_mean_squared_error: 0.3542 - val_loss: 0.1942 - val_root_mean_squared_error: 0.4407 - lr: 1.0000e-04 - 48s/epoch - 32ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "1518/1518 - 32s - loss: 0.0940 - root_mean_squared_error: 0.3066 - val_loss: 0.1877 - val_root_mean_squared_error: 0.4332 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "1518/1518 - 32s - loss: 0.0763 - root_mean_squared_error: 0.2763 - val_loss: 0.1946 - val_root_mean_squared_error: 0.4411 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "1518/1518 - 32s - loss: 0.0700 - root_mean_squared_error: 0.2645 - val_loss: 0.1891 - val_root_mean_squared_error: 0.4349 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "1518/1518 - 32s - loss: 0.0647 - root_mean_squared_error: 0.2544 - val_loss: 0.1838 - val_root_mean_squared_error: 0.4287 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "1518/1518 - 32s - loss: 0.0612 - root_mean_squared_error: 0.2473 - val_loss: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "1518/1518 - 32s - loss: 0.0576 - root_mean_squared_error: 0.2399 - val_loss: 0.1846 - val_root_mean_squared_error: 0.4296 - lr: 0.0010 - 32s/epoch - 21ms/step\n",
      "10-19 10:54:09 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "10-19 10:54:09 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "10-19 10:54:10 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241019104931_cin_nets_fm_nets_dnn_nets/cin_nets+fm_nets+dnn_nets.h5\n",
      "10-19 10:54:12 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:54:12 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:54:13 I deeptables.m.preprocessor.py 249 - transform_X taken 0.8778932094573975s\n",
      "10-19 10:54:13 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:54:13 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "76/76 [==============================] - 2s 14ms/step\n",
      "10-19 10:54:15 I deeptables.m.deeptable.py 559 - predict_proba taken 3.096867561340332s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 6 | rmse: 0.42980000376701355\n",
      "\n",
      "\u001b[34m\u001b[1mOverall CV rmse: 0.4343\n",
      "\n",
      "Data shape: (3, 597)\n",
      "\n",
      "Scaling 588 numerical cols.\n",
      "\n",
      "10-19 10:54:16 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:54:16 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:54:16 I deeptables.m.preprocessor.py 249 - transform_X taken 0.11453580856323242s\n",
      "10-19 10:54:16 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:54:16 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "10-19 10:54:16 I deeptables.m.deeptable.py 559 - predict_proba taken 0.4330570697784424s\n",
      "10-19 10:54:16 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:54:16 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:54:16 I deeptables.m.preprocessor.py 249 - transform_X taken 0.11855387687683105s\n",
      "10-19 10:54:16 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:54:16 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "10-19 10:54:16 I deeptables.m.deeptable.py 559 - predict_proba taken 0.3368866443634033s\n",
      "10-19 10:54:16 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:54:16 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:54:17 I deeptables.m.preprocessor.py 249 - transform_X taken 0.11813926696777344s\n",
      "10-19 10:54:17 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:54:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "10-19 10:54:17 I deeptables.m.deeptable.py 559 - predict_proba taken 0.34070444107055664s\n",
      "10-19 10:54:17 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:54:17 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:54:17 I deeptables.m.preprocessor.py 249 - transform_X taken 0.11793351173400879s\n",
      "10-19 10:54:17 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:54:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "10-19 10:54:17 I deeptables.m.deeptable.py 559 - predict_proba taken 0.3415384292602539s\n",
      "10-19 10:54:17 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:54:17 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:54:17 I deeptables.m.preprocessor.py 249 - transform_X taken 0.11744499206542969s\n",
      "10-19 10:54:17 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:54:17 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "10-19 10:54:17 I deeptables.m.deeptable.py 559 - predict_proba taken 0.34812331199645996s\n",
      "10-19 10:54:17 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "10-19 10:54:17 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "10-19 10:54:18 I deeptables.m.preprocessor.py 249 - transform_X taken 0.11502814292907715s\n",
      "10-19 10:54:18 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "10-19 10:54:18 I deeptables.u.dataset_generator.py 240 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "10-19 10:54:18 I deeptables.m.deeptable.py 559 - predict_proba taken 0.14908671379089355s\n",
      "CPU times: user 45min 51s, sys: 5min 4s, total: 50min 56s\n",
      "Wall time: 28min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run_i = 0\n",
    "scaler = CFG.scaler\n",
    "def predict(test_data, submission):\n",
    "    global run_i, scaler, models\n",
    "    if run_i == 0:\n",
    "        train_df = pl.read_csv(CFG.train_path)\n",
    "        train_df, num_cols = preprocess_data(train_df)\n",
    "        if scaler is not None:\n",
    "            print(f'Scaling {len(num_cols)} numerical cols.\\n')\n",
    "            train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "        models = train_nn(train_df)\n",
    "    run_i += 1\n",
    "    test_df, num_cols = preprocess_data(test_data)\n",
    "    test_df = test_df.drop(columns=game_col)\n",
    "    if scaler is not None:\n",
    "        print(f'Scaling {len(num_cols)} numerical cols.\\n')\n",
    "        test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "    return submission.with_columns(pl.Series(target_col, infer_nn(test_df, models)))\n",
    "\n",
    "inference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(predict)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv',\n",
    "         '/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbac9f",
   "metadata": {
    "papermill": {
     "duration": 0.045624,
     "end_time": "2024-10-19T10:54:18.248556",
     "exception": false,
     "start_time": "2024-10-19T10:54:18.202932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9515283,
     "sourceId": 70089,
     "sourceType": "competition"
    },
    {
     "datasetId": 4074593,
     "sourceId": 7074842,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4021289,
     "sourceId": 7570020,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5661348,
     "sourceId": 9341631,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1785.352014,
   "end_time": "2024-10-19T10:54:21.614526",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-19T10:24:36.262512",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
