{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":175528871,"sourceType":"kernelVersion"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n- Deberta-v3 xsmall huggingface version\n- Previous notebook is [here](https://www.kaggle.com/code/piantic/train-lmsys-deberta-v3-starter-code/notebook)\n- This notebook introduces the use of three texts. - `prompt`, `res_a`, `res_b`\n- Inference notebook will be released at a later date.\n\n\nIf this notebook is helpful, feel free to upvote.\n\nAnd please upvote the original notebook :)","metadata":{}},{"cell_type":"markdown","source":"`V1` - GPU T4x2 and initial settings\n\n`V2` - only use `prompt` because of problem where only 1 label appears during prediction\n\n`V3` - add truncate function for texts","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\nimport re\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\n\nos.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels transformers')\nos.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels tokenizers')\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import log_loss\nfrom tokenizers import AddedToken\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:56:04.430411Z","iopub.execute_input":"2024-06-16T02:56:04.431401Z","iopub.status.idle":"2024-06-16T02:56:55.457237Z","shell.execute_reply.started":"2024-06-16T02:56:04.431361Z","shell.execute_reply":"2024-06-16T02:56:55.455769Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: ../input/lmsys-pip-wheels\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nLooking in links: ../input/lmsys-pip-wheels\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.15.2)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n","output_type":"stream"},{"name":"stderr","text":"2024-06-16 02:56:45.049339: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-16 02:56:45.049471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-16 02:56:45.220631: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:56:55.459727Z","iopub.execute_input":"2024-06-16T02:56:55.461400Z","iopub.status.idle":"2024-06-16T02:56:55.472222Z","shell.execute_reply.started":"2024-06-16T02:56:55.461358Z","shell.execute_reply":"2024-06-16T02:56:55.465859Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ntest = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\nsubmission = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:56:55.474639Z","iopub.execute_input":"2024-06-16T02:56:55.475175Z","iopub.status.idle":"2024-06-16T02:56:59.051476Z","shell.execute_reply.started":"2024-06-16T02:56:55.475132Z","shell.execute_reply":"2024-06-16T02:56:59.050158Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:56:59.054936Z","iopub.execute_input":"2024-06-16T02:56:59.055827Z","iopub.status.idle":"2024-06-16T02:56:59.089109Z","shell.execute_reply.started":"2024-06-16T02:56:59.055783Z","shell.execute_reply":"2024-06-16T02:56:59.087846Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"               id             model_a              model_b  \\\n0           30192  gpt-4-1106-preview           gpt-4-0613   \n1           53567           koala-13b           gpt-4-0613   \n2           65089  gpt-3.5-turbo-0613       mistral-medium   \n3           96401    llama-2-13b-chat  mistral-7b-instruct   \n4          198779           koala-13b   gpt-3.5-turbo-0314   \n...           ...                 ...                  ...   \n57472  4294656694          gpt-4-0613             claude-1   \n57473  4294692063          claude-2.0     llama-2-13b-chat   \n57474  4294710549            claude-1           alpaca-13b   \n57475  4294899228              palm-2       tulu-2-dpo-70b   \n57476  4294947231  gemini-pro-dev-api   gpt-4-1106-preview   \n\n                                                  prompt  \\\n0      [\"Is it morally right to try to have a certain...   \n1      [\"What is the difference between marriage lice...   \n2      [\"explain function calling. how would you call...   \n3      [\"How can I create a test set for a very rare ...   \n4      [\"What is the best way to travel from Tel-Aviv...   \n...                                                  ...   \n57472  [\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...   \n57473  [\"In python, implement a naive Bayes with gaus...   \n57474  [\"is it unethical to work on building weapons?...   \n57475  [\"If a bait contains 0,0025% bromadiolon then ...   \n57476  [\"three kids eat three apples in three days, h...   \n\n                                              response_a  \\\n0      [\"The question of whether it is morally right ...   \n1      [\"A marriage license is a legal document that ...   \n2      [\"Function calling is the process of invoking ...   \n3      [\"Creating a test set for a very rare category...   \n4      [\"The best way to travel from Tel Aviv to Jeru...   \n...                                                  ...   \n57472  [\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...   \n57473  [\"Here is an implementation of a naive Bayes c...   \n57474  [\"Working on weapons technology raises some et...   \n57475  [\"Bromadiolone is a rodenticide which is most ...   \n57476                                      [\"27 apples\"]   \n\n                                              response_b  winner_model_a  \\\n0      [\"As an AI, I don't have personal beliefs or o...               1   \n1      [\"A marriage license and a marriage certificat...               0   \n2      [\"Function calling is the process of invoking ...               0   \n3      [\"When building a classifier for a very rare c...               1   \n4      [\"The best way to travel from Tel-Aviv to Jeru...               0   \n...                                                  ...             ...   \n57472  [\"Here is how that mnemonic represents the dig...               1   \n57473  [\"Sure! Here's an implementation of a naive Ba...               1   \n57474  [\"It depends on the context. Weapons can be us...               1   \n57475  [\"As an AI language model, I do not promote or...               0   \n57476  [\"If three kids eat three apples in three days...               1   \n\n       winner_model_b  winner_tie  \n0                   0           0  \n1                   1           0  \n2                   0           1  \n3                   0           0  \n4                   1           0  \n...               ...         ...  \n57472               0           0  \n57473               0           0  \n57474               0           0  \n57475               1           0  \n57476               0           0  \n\n[57477 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>57472</th>\n      <td>4294656694</td>\n      <td>gpt-4-0613</td>\n      <td>claude-1</td>\n      <td>[\"A simple mnemonic for \\u03c0:\\n\\\"How I wish ...</td>\n      <td>[\"Sure, let's break it down:\\n\\n1. \\\"How\\\" has...</td>\n      <td>[\"Here is how that mnemonic represents the dig...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57473</th>\n      <td>4294692063</td>\n      <td>claude-2.0</td>\n      <td>llama-2-13b-chat</td>\n      <td>[\"In python, implement a naive Bayes with gaus...</td>\n      <td>[\"Here is an implementation of a naive Bayes c...</td>\n      <td>[\"Sure! Here's an implementation of a naive Ba...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57474</th>\n      <td>4294710549</td>\n      <td>claude-1</td>\n      <td>alpaca-13b</td>\n      <td>[\"is it unethical to work on building weapons?...</td>\n      <td>[\"Working on weapons technology raises some et...</td>\n      <td>[\"It depends on the context. Weapons can be us...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57475</th>\n      <td>4294899228</td>\n      <td>palm-2</td>\n      <td>tulu-2-dpo-70b</td>\n      <td>[\"If a bait contains 0,0025% bromadiolon then ...</td>\n      <td>[\"Bromadiolone is a rodenticide which is most ...</td>\n      <td>[\"As an AI language model, I do not promote or...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>57476</th>\n      <td>4294947231</td>\n      <td>gemini-pro-dev-api</td>\n      <td>gpt-4-1106-preview</td>\n      <td>[\"three kids eat three apples in three days, h...</td>\n      <td>[\"27 apples\"]</td>\n      <td>[\"If three kids eat three apples in three days...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>57477 rows Ã— 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class CFG:\n    n_splits = 5\n    seed = 42\n    max_length = 1539 # 512 x 3 + a\n    lr = 1e-5\n    train_batch_size = 8\n    eval_batch_size = 4\n    train_epochs = 4\n    weight_decay = 0.01\n    warmup_ratio = 0.1\n    num_labels = 3\n    debug=False\n    model = \"microsoft/deberta-v3-xsmall\"\n    target_cols = ['winner_model_a', 'winner_model_b', 'winner_tie']","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:56:59.090691Z","iopub.execute_input":"2024-06-16T02:56:59.091154Z","iopub.status.idle":"2024-06-16T02:56:59.098110Z","shell.execute_reply.started":"2024-06-16T02:56:59.091109Z","shell.execute_reply":"2024-06-16T02:56:59.097029Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Add Features","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/piantic/train-lmsys-deberta-v3-starter-code/notebook\n\ndef add_label(df):\n    labels = np.zeros(len(df), dtype=np.int32)\n    labels[df['winner_model_a'] == 1] = 0\n    labels[df['winner_model_b'] == 1] = 1\n    labels[df['winner_tie'] == 1] = 2\n    df['label'] = labels\n    return df\n\n\ndef add_stats(df):\n    # Some stats\n    df[\"prompt_words\"] = df[\"prompt\"].apply(lambda x: x.replace('\\n', ' ').split(\" \"))\n    df[\"total_prompt_words\"] = df[\"prompt\"].apply(lambda x: len(x.split(\" \")))\n    df[\"prompt_length\"] = df[\"prompt\"].apply(lambda x: len(x))\n\n    df[\"response_a_words\"] = df[\"response_a\"].apply(lambda x: x.replace('\\n', ' ').split(\" \"))\n    df[\"total_response_a_words\"] = df[\"response_a\"].apply(lambda x: len(x.split(\" \")))\n    df[\"response_a_length\"] = df[\"response_a\"].apply(lambda x: len(x))\n\n    df[\"response_b_words\"] = df[\"response_b\"].apply(lambda x: x.replace('\\n', ' ').split(\" \"))\n    df[\"total_response_b_words\"] = df[\"response_b\"].apply(lambda x: len(x.split(\" \")))\n    df[\"response_b_length\"] = df[\"response_b\"].apply(lambda x: len(x))\n    \n    return df\n\ndef truncate_text(df, column_name, max_length=1000000):\n    df[f\"{column_name}\"] = df[column_name].str[:max_length]\n    return df\n\ntrain = add_label(train)\ntrain = add_stats(train)\ntrain = truncate_text(train, 'prompt')\ntrain = truncate_text(train, 'response_a')\ntrain = truncate_text(train, 'response_b')","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:56:59.099428Z","iopub.execute_input":"2024-06-16T02:56:59.099776Z","iopub.status.idle":"2024-06-16T02:57:05.580952Z","shell.execute_reply.started":"2024-06-16T02:56:59.099738Z","shell.execute_reply":"2024-06-16T02:57:05.579887Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization\n- In this notebook, i will use `prompt`, `response_a`, `response_b`.","metadata":{}},{"cell_type":"code","source":"class Tokenize(object):\n    def __init__(self, train, valid):\n        self.tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n        self.train = train\n        self.valid = valid\n        \n    def get_dataset(self, df):\n        ds = Dataset.from_dict({\"\"\n                'id': [e for e in df['id']],\n                'prompt': [ft for ft in df['prompt']],\n                'response_a': [ft for ft in df['response_a']],\n                'response_b': [ft for ft in df['response_b']],\n                'labels': [s for s in df['label']],\n            })\n        return ds\n    \n    def tokenize_function(self, df):\n        tokenized_inputs = self.tokenizer(\n            df['prompt'], df['response_a'] + self.tokenizer.sep_token + df['response_b'],\n            truncation=True, padding=True, max_length=CFG.max_length\n        )\n        return tokenized_inputs\n    \n    def __call__(self):\n        train_ds = self.get_dataset(train)\n        valid_ds = self.get_dataset(valid)\n        \n        tokenized_train = train_ds.map(\n            self.tokenize_function\n        )\n        tokenized_valid = valid_ds.map(\n            self.tokenize_function\n        )\n        \n        return tokenized_train, tokenized_valid, self.tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-06-16T03:01:27.840641Z","iopub.execute_input":"2024-06-16T03:01:27.841069Z","iopub.status.idle":"2024-06-16T03:01:27.854993Z","shell.execute_reply.started":"2024-06-16T03:01:27.841037Z","shell.execute_reply":"2024-06-16T03:01:27.853966Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Compute metrics","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    score = log_loss(labels, predictions)\n    results = {\n        'score': score\n    }\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:57:05.596152Z","iopub.execute_input":"2024-06-16T02:57:05.596517Z","iopub.status.idle":"2024-06-16T02:57:05.611299Z","shell.execute_reply.started":"2024-06-16T02:57:05.596490Z","shell.execute_reply":"2024-06-16T02:57:05.610238Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# CV split","metadata":{}},{"cell_type":"code","source":"data = train.copy()\ndata[\"label\"] = data[\"label\"].astype('int32')\nskf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\nfor i, (_, val_index) in enumerate(skf.split(data, data[\"label\"])):\n    data.loc[val_index, \"fold\"] = i\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:57:12.478478Z","iopub.execute_input":"2024-06-16T02:57:12.478878Z","iopub.status.idle":"2024-06-16T02:57:12.595836Z","shell.execute_reply.started":"2024-06-16T02:57:12.478846Z","shell.execute_reply":"2024-06-16T02:57:12.594744Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  label  \\\n0               0           0      0   \n1               1           0      1   \n2               0           1      2   \n3               0           0      0   \n4               1           0      1   \n\n                                        prompt_words  total_prompt_words  \\\n0  [[\"Is, it, morally, right, to, try, to, have, ...                  28   \n1  [[\"What, is, the, difference, between, marriag...                  35   \n2  [[\"explain, function, calling., how, would, yo...                   9   \n3  [[\"How, can, I, create, a, test, set, for, a, ...                  18   \n4  [[\"What, is, the, best, way, to, travel, from,...                  14   \n\n   prompt_length                                   response_a_words  \\\n0            165  [[\"The, question, of, whether, it, is, morally...   \n1            200  [[\"A, marriage, license, is, a, legal, documen...   \n2             60  [[\"Function, calling, is, the, process, of, in...   \n3             87  [[\"Creating, a, test, set, for, a, very, rare,...   \n4             79  [[\"The, best, way, to, travel, from, Tel, Aviv...   \n\n   total_response_a_words  response_a_length  \\\n0                     656               4538   \n1                     537               3114   \n2                     141                921   \n3                     536               3182   \n4                     236               1300   \n\n                                    response_b_words  total_response_b_words  \\\n0  [[\"As, an, AI,, I, don't, have, personal, beli...                     204   \n1  [[\"A, marriage, license, and, a, marriage, cer...                     591   \n2  [[\"Function, calling, is, the, process, of, in...                     282   \n3  [[\"When, building, a, classifier, for, a, very...                     265   \n4  [[\"The, best, way, to, travel, from, Tel-Aviv,...                     122   \n\n   response_b_length  fold  \n0               1206   4.0  \n1               3649   3.0  \n2               1835   3.0  \n3               1562   0.0  \n4                772   3.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>model_a</th>\n      <th>model_b</th>\n      <th>prompt</th>\n      <th>response_a</th>\n      <th>response_b</th>\n      <th>winner_model_a</th>\n      <th>winner_model_b</th>\n      <th>winner_tie</th>\n      <th>label</th>\n      <th>prompt_words</th>\n      <th>total_prompt_words</th>\n      <th>prompt_length</th>\n      <th>response_a_words</th>\n      <th>total_response_a_words</th>\n      <th>response_a_length</th>\n      <th>response_b_words</th>\n      <th>total_response_b_words</th>\n      <th>response_b_length</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30192</td>\n      <td>gpt-4-1106-preview</td>\n      <td>gpt-4-0613</td>\n      <td>[\"Is it morally right to try to have a certain...</td>\n      <td>[\"The question of whether it is morally right ...</td>\n      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[[\"Is, it, morally, right, to, try, to, have, ...</td>\n      <td>28</td>\n      <td>165</td>\n      <td>[[\"The, question, of, whether, it, is, morally...</td>\n      <td>656</td>\n      <td>4538</td>\n      <td>[[\"As, an, AI,, I, don't, have, personal, beli...</td>\n      <td>204</td>\n      <td>1206</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53567</td>\n      <td>koala-13b</td>\n      <td>gpt-4-0613</td>\n      <td>[\"What is the difference between marriage lice...</td>\n      <td>[\"A marriage license is a legal document that ...</td>\n      <td>[\"A marriage license and a marriage certificat...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[[\"What, is, the, difference, between, marriag...</td>\n      <td>35</td>\n      <td>200</td>\n      <td>[[\"A, marriage, license, is, a, legal, documen...</td>\n      <td>537</td>\n      <td>3114</td>\n      <td>[[\"A, marriage, license, and, a, marriage, cer...</td>\n      <td>591</td>\n      <td>3649</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>65089</td>\n      <td>gpt-3.5-turbo-0613</td>\n      <td>mistral-medium</td>\n      <td>[\"explain function calling. how would you call...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>[\"Function calling is the process of invoking ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>[[\"explain, function, calling., how, would, yo...</td>\n      <td>9</td>\n      <td>60</td>\n      <td>[[\"Function, calling, is, the, process, of, in...</td>\n      <td>141</td>\n      <td>921</td>\n      <td>[[\"Function, calling, is, the, process, of, in...</td>\n      <td>282</td>\n      <td>1835</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>96401</td>\n      <td>llama-2-13b-chat</td>\n      <td>mistral-7b-instruct</td>\n      <td>[\"How can I create a test set for a very rare ...</td>\n      <td>[\"Creating a test set for a very rare category...</td>\n      <td>[\"When building a classifier for a very rare c...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[[\"How, can, I, create, a, test, set, for, a, ...</td>\n      <td>18</td>\n      <td>87</td>\n      <td>[[\"Creating, a, test, set, for, a, very, rare,...</td>\n      <td>536</td>\n      <td>3182</td>\n      <td>[[\"When, building, a, classifier, for, a, very...</td>\n      <td>265</td>\n      <td>1562</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198779</td>\n      <td>koala-13b</td>\n      <td>gpt-3.5-turbo-0314</td>\n      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[[\"What, is, the, best, way, to, travel, from,...</td>\n      <td>14</td>\n      <td>79</td>\n      <td>[[\"The, best, way, to, travel, from, Tel, Aviv...</td>\n      <td>236</td>\n      <td>1300</td>\n      <td>[[\"The, best, way, to, travel, from, Tel-Aviv,...</td>\n      <td>122</td>\n      <td>772</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"if CFG.debug:\n    display(data.groupby('fold').size())\n    data = data.sample(n=1000, random_state=0).reset_index(drop=True)\n    display(data.groupby('fold').size())","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:57:14.846990Z","iopub.execute_input":"2024-06-16T02:57:14.847385Z","iopub.status.idle":"2024-06-16T02:57:14.853558Z","shell.execute_reply.started":"2024-06-16T02:57:14.847348Z","shell.execute_reply":"2024-06-16T02:57:14.852319Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Set Training Args","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    fp16=True,\n    learning_rate=CFG.lr,\n    per_device_train_batch_size=CFG.train_batch_size,\n    per_device_eval_batch_size=CFG.eval_batch_size,\n    num_train_epochs=CFG.train_epochs,\n    weight_decay=CFG.weight_decay,\n    evaluation_strategy='epoch',\n    metric_for_best_model='score',\n    save_strategy='epoch',\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    report_to='none',\n    warmup_ratio=CFG.warmup_ratio,\n    optim='adamw_torch'\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T02:57:21.172364Z","iopub.execute_input":"2024-06-16T02:57:21.173593Z","iopub.status.idle":"2024-06-16T02:57:21.300111Z","shell.execute_reply.started":"2024-06-16T02:57:21.173554Z","shell.execute_reply":"2024-06-16T02:57:21.299151Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Training by hold","metadata":{}},{"cell_type":"code","source":"for fold in range(len(data['fold'].unique())):\n    train = data[data['fold'] != fold]\n    valid = data[data['fold'] == fold]\n    \n    # ADD NEW TOKENS for (\"\\n\") new paragraph and (\" \"*2) double space\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n    tokenizer.add_tokens([AddedToken(\"\\n\", normalized=False)])\n    tokenizer.add_tokens([AddedToken(\" \"*2, normalized=False)])\n    tokenize = Tokenize(train, valid)\n    tokenized_train, tokenized_valid, tokenizer = tokenize()\n    \n    model = AutoModelForSequenceClassification.from_pretrained(CFG.model, num_labels=CFG.num_labels)\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_valid,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    \n    trainer.train()\n    \n    y_true = valid['label'].values\n    predictions = trainer.predict(tokenized_valid).predictions\n    predictions = predictions.argmax(axis=1)\n    cm = confusion_matrix(y_true, predictions, labels=[x for x in range(0,3)])\n    draw_cm = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=[x for x in range(0,3)])\n    draw_cm.plot()\n    plt.show()\n    \n    trainer.save_model(f'deberta-v3-xsmall_fold_{fold}')\n    tokenizer.save_pretrained(f'deberta-v3-xsmall_fold_{fold}')\n    \n    valid.to_csv(f'valid_df_fold_{fold}.csv', index=False)\n    break # just fold 0 train for test","metadata":{"execution":{"iopub.status.busy":"2024-06-16T03:01:34.735205Z","iopub.execute_input":"2024-06-16T03:01:34.735984Z","iopub.status.idle":"2024-06-16T03:11:17.859432Z","shell.execute_reply.started":"2024-06-16T03:01:34.735948Z","shell.execute_reply":"2024-06-16T03:11:17.857756Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/45981 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1843a4db53f467f8e1984c890be0c58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11496 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5467083ada24559b8d3ebf81652339a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/241M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"631212f44916441a8537d2e7979e2eac"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     13\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m y_true \u001b[38;5;241m=\u001b[39m valid[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     28\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(tokenized_valid)\u001b[38;5;241m.\u001b[39mpredictions\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 1300, in forward\n    outputs = self.deberta(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 1070, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 514, in forward\n    output_states = layer_module(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 362, in forward\n    attention_output = self.attention(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 293, in forward\n    self_output = self.self(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 721, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 795, in disentangled_attention_bias\n    c2p_att = torch.gather(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 434.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 215.06 MiB is free. Process 5651 has 14.54 GiB memory in use. Of the allocated memory 13.87 GiB is allocated by PyTorch, and 478.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"],"ename":"OutOfMemoryError","evalue":"Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 1300, in forward\n    outputs = self.deberta(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 1070, in forward\n    encoder_outputs = self.encoder(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 514, in forward\n    output_states = layer_module(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 362, in forward\n    attention_output = self.attention(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 293, in forward\n    self_output = self.self(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 721, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 795, in disentangled_attention_bias\n    c2p_att = torch.gather(\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 434.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 215.06 MiB is free. Process 5651 has 14.54 GiB memory in use. Of the allocated memory 13.87 GiB is allocated by PyTorch, and 478.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","output_type":"error"}]},{"cell_type":"code","source":"tokenized_train[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-16T03:11:43.859071Z","iopub.execute_input":"2024-06-16T03:11:43.860292Z","iopub.status.idle":"2024-06-16T03:11:43.932915Z","shell.execute_reply.started":"2024-06-16T03:11:43.860254Z","shell.execute_reply":"2024-06-16T03:11:43.931857Z"},"scrolled":true,"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'id': 30192,\n 'prompt': '[\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"]',\n 'response_a': '[\"The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\\\n\\\\nHere are some arguments in favor of and against such policies:\\\\n\\\\n**Arguments in favor:**\\\\n\\\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\\\n\\\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\\\n\\\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\\\n\\\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\\\n\\\\n**Arguments against:**\\\\n\\\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\\\n\\\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\\\n\\\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \\\\\"tokens,\\\\\" undermining their legitimacy and potentially leading to resentment among colleagues.\\\\n\\\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\\\n\\\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one\\'s ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \\\\\"color-blind\\\\\" or \\\\\"gender-blind\\\\\" approach to hiring and promotions.\\\\n\\\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.\",\"Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\\\\n\\\\nLet\\'s slice into the debate with a zest of fun:\\\\n\\\\n**Team Pineapple:** \\\\\"Absolutely, yes! Pineapple on pizza is like a beach party in your mouth. The sweet juiciness of pineapple chunks frolicking with savory ham or bacon creates a flavor wave that surfs across the cheesy ocean of deliciousness. It\\'s the Mardi Gras of pizzas, where sweet meets savory in a jubilant jamboree!\\\\\"\\\\n\\\\n**Team No-Pineapple:** \\\\\"No way, not in a million pizzas! Pineapple is a fruit that should be sunbathing on a tropical fruit platter, not freeloading on a sacred slice of pizza. The thought of warm, melty cheese conspiring with pineapple\\'s sugary mischief is enough to make Italian ancestors turn in their tomato patches. Keep the pineapple in the fruit salad where it belongs!\\\\\"\\\\n\\\\nAt the end of the day, whether pineapple belongs on pizza is a matter of personal taste. Some say it\\'s a match made in heaven, others say it\\'s a culinary crime. But let\\'s face it, in the world of pizza toppings, where you can find everything from anchovies to zucchini, there\\'s room for a little pineapple pizzazz. So, if your taste buds crave that sweet and tangy twist, go ahead and let your pineapple flag fly atop that glorious cheese-covered dough. Bon app\\\\u00e9tit, or as they say in pineapple paradise, \\\\\"Aloha-ppetite!\\\\\" \\\\ud83c\\\\udf4d\\\\ud83c\\\\udf55\"]',\n 'response_b': '[\"As an AI, I don\\'t have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It\\'s a topic with valid arguments on both sides, and it\\'s up to each individual or organization to decide what they believe is right.\",\"As an AI, I don\\'t eat, so I don\\'t have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It\\'s a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!\"]',\n 'labels': 0,\n 'input_ids': [1,\n  647,\n  309,\n  12977,\n  278,\n  21090,\n  423,\n  264,\n  687,\n  264,\n  286,\n  266,\n  991,\n  3623,\n  265,\n  9661,\n  277,\n  23594,\n  3129,\n  302,\n  309,\n  261,\n  309,\n  14399,\n  261,\n  490,\n  16550,\n  6697,\n  277,\n  266,\n  5696,\n  302,\n  22838,\n  263,\n  527,\n  351,\n  785,\n  1330,\n  260,\n  309,\n  592,\n  2,\n  647,\n  309,\n  635,\n  900,\n  265,\n  786,\n  278,\n  269,\n  21090,\n  423,\n  264,\n  3140,\n  270,\n  266,\n  991,\n  3623,\n  265,\n  9661,\n  267,\n  23594,\n  3129,\n  269,\n  266,\n  1739,\n  7431,\n  889,\n  272,\n  3960,\n  10618,\n  265,\n  16063,\n  261,\n  8650,\n  261,\n  4603,\n  261,\n  263,\n  7384,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  15816,\n  281,\n  347,\n  6025,\n  267,\n  4284,\n  265,\n  263,\n  532,\n  405,\n  2294,\n  294,\n  2620,\n  673,\n  2620,\n  673,\n  1225,\n  1225,\n  114630,\n  268,\n  267,\n  4284,\n  294,\n  1225,\n  1225,\n  2620,\n  673,\n  2620,\n  673,\n  435,\n  260,\n  1124,\n  1225,\n  73844,\n  510,\n  9801,\n  344,\n  71002,\n  7643,\n  294,\n  1225,\n  1225,\n  2568,\n  286,\n  10307,\n  331,\n  43027,\n  267,\n  2297,\n  3930,\n  775,\n  264,\n  847,\n  2546,\n  261,\n  7497,\n  261,\n  263,\n  728,\n  7584,\n  260,\n  82202,\n  270,\n  266,\n  996,\n  3623,\n  295,\n  282,\n  757,\n  283,\n  266,\n  24099,\n  2520,\n  264,\n  990,\n  695,\n  263,\n  3323,\n  7384,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  445,\n  260,\n  1124,\n  1225,\n  110275,\n  5654,\n  18464,\n  294,\n  1225,\n  1225,\n  54206,\n  2297,\n  1737,\n  295,\n  3337,\n  1129,\n  271,\n  4898,\n  263,\n  2993,\n  266,\n  7206,\n  778,\n  265,\n  8974,\n  260,\n  329,\n  295,\n  917,\n  264,\n  493,\n  4338,\n  270,\n  2035,\n  263,\n  1846,\n  283,\n  266,\n  783,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  508,\n  260,\n  1124,\n  1225,\n  1276,\n  4380,\n  265,\n  16730,\n  294,\n  1225,\n  1225,\n  14708,\n  4786,\n  270,\n  2416,\n  5190,\n  267,\n  895,\n  295,\n  408,\n  1065,\n  272,\n  694,\n  286,\n  3100,\n  1604,\n  264,\n  2536,\n  267,\n  308,\n  7335,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  554,\n  260,\n  1124,\n  1225,\n  78871,\n  22988,\n  294,\n  1225,\n  1225,\n  21772,\n  8051,\n  265,\n  2416,\n  1840,\n  295,\n  5984,\n  263,\n  2700,\n  340,\n  694,\n  263,\n  1973,\n  264,\n  5012,\n  2297,\n  3930,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  1225,\n  1225,\n  114630,\n  268,\n  532,\n  294,\n  1225,\n  1225,\n  2620,\n  673,\n  2620,\n  673,\n  435,\n  260,\n  1124,\n  1225,\n  113807,\n  42816,\n  294,\n  1225,\n  1225,\n  14708,\n  266,\n  22598,\n  270,\n  2416,\n  5190,\n  520,\n  917,\n  264,\n  262,\n  6720,\n  289,\n  1901,\n  265,\n  5315,\n  7384,\n  261,\n  399,\n  842,\n  520,\n  282,\n  11118,\n  270,\n  3129,\n  2128,\n  411,\n  3573,\n  261,\n  891,\n  264,\n  957,\n  262,\n  3790,\n  1782,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  445,\n  260,\n  1124,\n  1225,\n  118006,\n  50295,\n  41956,\n  294,\n  1225,\n  1225,\n  20021,\n  5118,\n  272,\n  3192,\n  263,\n  4698,\n  403,\n  282,\n  636,\n  5847,\n  277,\n  11328,\n  263,\n  9964,\n  261,\n  298,\n  3790,\n  260,\n  450,\n  2189,\n  272,\n  35772,\n  387,\n  7787,\n  262,\n  607,\n  265,\n  2297,\n  337,\n  625,\n  3573,\n  1500,\n  281,\n  7559,\n  264,\n  957,\n  3790,\n  4786,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  508,\n  260,\n  1124,\n  1225,\n  70507,\n  2934,\n  294,\n  1225,\n  1225,\n  443,\n  269,\n  266,\n  1051,\n  272,\n  694,\n  4368,\n  264,\n  957,\n  35772,\n  520,\n  282,\n  757,\n  283,\n  9497,\n  309,\n  56218,\n  268,\n  261,\n  2620,\n  309,\n  29702,\n  308,\n  20314,\n  263,\n  3695,\n  1249,\n  264,\n  22371,\n  880,\n  4331,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  554,\n  260,\n  1124,\n  1225,\n  17177,\n  268,\n  67418,\n  13011,\n  265,\n  18464,\n  294,\n  1225,\n  1225,\n  31543,\n  364,\n  277,\n  3790,\n  520,\n  15794,\n  340,\n  539,\n  2592,\n  265,\n  4603,\n  261,\n  405,\n  283,\n  1583,\n  261,\n  19859,\n  261,\n  27377,\n  2008,\n  261,\n  289,\n  2749,\n  8100,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  87354,\n  261,\n  262,\n  15481,\n  265,\n  10528,\n  270,\n  266,\n  991,\n  3623,\n  265,\n  9661,\n  267,\n  23594,\n  3129,\n  3898,\n  277,\n  311,\n  280,\n  268,\n  7431,\n  4015,\n  263,\n  262,\n  2758,\n  267,\n  319,\n  405,\n  2294,\n  281,\n  4579,\n  260,\n  66222,\n  265,\n  3790,\n  4603,\n  4786,\n  611,\n  5118,\n  272,\n  378,\n  2588,\n  281,\n  1241,\n  283,\n  266,\n  19584,\n  4866,\n  264,\n  676,\n  266,\n  674,\n  1185,\n  946,\n  261,\n  438,\n  7207,\n  372,\n  5118,\n  270,\n  266,\n  9497,\n  309,\n  11772,\n  271,\n  23709,\n  2620,\n  309,\n  289,\n  9497,\n  309,\n  36263,\n  271,\n  23709,\n  2620,\n  309,\n  1218,\n  264,\n  5108,\n  263,\n  9426,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  83604,\n  268,\n  263,\n  9697,\n  516,\n  3153,\n  1268,\n  262,\n  6634,\n  265,\n  378,\n  2638,\n  263,\n  6525,\n  264,\n  3923,\n  2294,\n  272,\n  2655,\n  16063,\n  263,\n  8650,\n  438,\n  20596,\n  2330,\n  4250,\n  260,\n  325,\n  269,\n  327,\n  539,\n  264,\n  990,\n  262,\n  3955,\n  2884,\n  265,\n  3790,\n  12570,\n  261,\n  405,\n  283,\n  13686,\n  7958,\n  261,\n  374,\n  271,\n  4833,\n  1990,\n  2070,\n  261,\n  263,\n  1838,\n  265,\n  7189,\n  2294,\n  261,\n  264,\n  676,\n  299,\n  1192,\n  399,\n  837,\n  303,\n  262,\n  1062,\n  264,\n  5376,\n  636,\n  277,\n  308,\n  4070,\n  263,\n  4782,\n  260,\n  309,\n  261,\n  309,\n  24433,\n  261,\n  262,\n  905,\n  271,\n  1371,\n  11121,\n  39273,\n  272,\n  303,\n  4888,\n  4706,\n  263,\n  2195,\n  3943,\n  294,\n  490,\n  16550,\n  6697,\n  277,\n  266,\n  5696,\n  302,\n  279,\n  7176,\n  6744,\n  265,\n  16550,\n  277,\n  5696,\n  261,\n  756,\n  283,\n  14813,\n  5696,\n  261,\n  269,\n  266,\n  43718,\n  19587,\n  2270,\n  399,\n  2448,\n  16024,\n  2587,\n  263,\n  5696,\n  72520,\n  5933,\n  260,\n  2620,\n  673,\n  2620,\n  673,\n  9975,\n  280,\n  268,\n  8774,\n  352,\n  262,\n  3389,\n  275,\n  266,\n  22855,\n  265,\n  785,\n  294,\n  2620,\n  673,\n  2620,\n  673,\n  1225,\n  1225,\n  22816,\n  40410,\n  294,\n  1225,\n  1225,\n  9497,\n  309,\n  65949,\n  261,\n  2489,\n  300,\n  40410,\n  277,\n  5696,\n  269,\n  334,\n  266,\n  2334,\n  915,\n  267,\n  290,\n  2933,\n  260,\n  279,\n  2172,\n  18350,\n  3116,\n  8353,\n  265,\n  16550,\n  15986,\n  90895,\n  275,\n  21937,\n  11965,\n  289,\n  9494,\n  3788,\n  266,\n  4481,\n  4195,\n  272,\n  10873,\n  268,\n  679,\n  262,\n  22664,\n  4383,\n  265,\n  63779,\n  260,\n  325,\n  280,\n  268,\n  262,\n  32589,\n  27720,\n  265,\n  27475,\n  261,\n  399,\n  2172,\n  4694,\n  21937,\n  267,\n  266,\n  76236,\n  8627,\n  40850,\n  473,\n  300,\n  2620,\n  309,\n  2620,\n  673,\n  2620,\n  673,\n  1225,\n  1225,\n  22816,\n  585,\n  271,\n  87954,\n  26847,\n  294,\n  1225,\n  1225,\n  9497,\n  309,\n  3724,\n  384,\n  261,\n  298,\n  267,\n  266,\n  705,\n  27475,\n  300,\n  40410,\n  269,\n  266,\n  2879,\n  272,\n  403,\n  282,\n  57287,\n  277,\n  266,\n  7176,\n  2879,\n  24700,\n  261,\n  298,\n  484,\n  30343,\n  277,\n  266,\n  8754,\n  8774,\n  265,\n  5696,\n  260,\n  279,\n  708,\n  265,\n  1926,\n  261,\n  10424,\n  608,\n  3188,\n  50484,\n  275,\n  16550,\n  280,\n  268,\n  28064,\n  31187,\n  269,\n  618,\n  264,\n  365,\n  3147,\n  11835,\n  930,\n  267,\n  308,\n  8570,\n  10432,\n  260,\n  2876,\n  262,\n  16550,\n  267,\n  262,\n  2879,\n  5826,\n  399,\n  278,\n  8543,\n  300,\n  2620,\n  309,\n  2620,\n  673,\n  2620,\n  673,\n  8880,\n  262,\n  513,\n  265,\n  262,\n  406,\n  261,\n  786,\n  16550,\n  8543,\n  277,\n  5696,\n  269,\n  266,\n  912,\n  265,\n  753,\n  2448,\n  260,\n  879,\n  504,\n  278,\n  280,\n  268,\n  266,\n  1511,\n  412,\n  267,\n  5940,\n  261,\n  690,\n  504,\n  278,\n  280,\n  268,\n  266,\n  11121,\n  2898,\n  260,\n  420,\n  678,\n  280,\n  268,\n  812,\n  278,\n  261,\n  267,\n  262,\n  447,\n  265,\n  5696,\n  26212,\n  261,\n  399,\n  274,\n  295,\n  433,\n  758,\n  292,\n  75141,\n  264,\n  23639,\n  261,\n  343,\n  280,\n  268,\n  629,\n  270,\n  266,\n  480,\n  16550,\n  93075,\n  260,\n  471,\n  261,\n  337,\n  290,\n  2448,\n  16024,\n  20206,\n  272,\n  2172,\n  263,\n  34265,\n  6744,\n  261,\n  424,\n  1645,\n  263,\n  678,\n  290,\n  16550,\n  4862,\n  3247,\n  14182,\n  272,\n  10236,\n  3188,\n  271,\n  19824,\n  6737,\n  260,\n  11988,\n  1410,\n  2620,\n  1964,\n  962,\n  473,\n  1088,\n  297,\n  1632,\n  261,\n  289,\n  283,\n  306,\n  504,\n  267,\n  ...],\n 'token_type_ids': [0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  ...],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  ...]}"},"metadata":{}}]},{"cell_type":"markdown","source":"# References\n- https://www.kaggle.com/code/hashidoyuto/deberta-baseline-aes2-0-train\n- https://www.kaggle.com/code/cdeotte/deberta-v3-small-starter-cv-0-820-lb-0-800?scriptVersionId=174239814","metadata":{}}]}