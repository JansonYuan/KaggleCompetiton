{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":175528871,"sourceType":"kernelVersion"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n- Deberta-v3 xsmall huggingface version\n- Previous notebook is [here](https://www.kaggle.com/code/piantic/train-lmsys-deberta-v3-starter-code/notebook)\n- This notebook introduces the use of three texts. - `prompt`, `res_a`, `res_b`\n- Inference notebook will be released at a later date.\n\n\nIf this notebook is helpful, feel free to upvote.\n\nAnd please upvote the original notebook :)","metadata":{}},{"cell_type":"markdown","source":"`V1` - GPU T4x2 and initial settings\n\n`V2` - only use `prompt` because of problem where only 1 label appears during prediction\n\n`V3` - add truncate function for texts","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\nimport re\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\n\nos.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels transformers')\nos.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels tokenizers')\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import DataCollatorWithPadding\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import log_loss\nfrom tokenizers import AddedToken\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:07:58.056458Z","iopub.execute_input":"2024-05-05T18:07:58.05686Z","iopub.status.idle":"2024-05-05T18:08:28.628914Z","shell.execute_reply.started":"2024-05-05T18:07:58.056817Z","shell.execute_reply":"2024-05-05T18:08:28.627938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:28.633841Z","iopub.execute_input":"2024-05-05T18:08:28.634121Z","iopub.status.idle":"2024-05-05T18:08:28.63921Z","shell.execute_reply.started":"2024-05-05T18:08:28.634096Z","shell.execute_reply":"2024-05-05T18:08:28.638288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ntest = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\nsubmission = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:28.640418Z","iopub.execute_input":"2024-05-05T18:08:28.641072Z","iopub.status.idle":"2024-05-05T18:08:30.489665Z","shell.execute_reply.started":"2024-05-05T18:08:28.641037Z","shell.execute_reply":"2024-05-05T18:08:30.48861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    n_splits = 5\n    seed = 42\n    max_length = 1539 # 512 x 3 + a\n    lr = 1e-5\n    train_batch_size = 8\n    eval_batch_size = 4\n    train_epochs = 4\n    weight_decay = 0.01\n    warmup_ratio = 0.1\n    num_labels = 3\n    debug=True\n    model = \"microsoft/deberta-v3-xsmall\"\n    target_cols = ['winner_model_a', 'winner_model_b', 'winner_tie']","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:30.492543Z","iopub.execute_input":"2024-05-05T18:08:30.49287Z","iopub.status.idle":"2024-05-05T18:08:30.498466Z","shell.execute_reply.started":"2024-05-05T18:08:30.492843Z","shell.execute_reply":"2024-05-05T18:08:30.497467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add Features","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/piantic/train-lmsys-deberta-v3-starter-code/notebook\n\ndef add_label(df):\n    labels = np.zeros(len(df), dtype=np.int32)\n    labels[df['winner_model_a'] == 1] = 0\n    labels[df['winner_model_b'] == 1] = 1\n    labels[df['winner_tie'] == 1] = 2\n    df['label'] = labels\n    return df\n\n\ndef add_stats(df):\n    # Some stats\n    df[\"prompt_words\"] = df[\"prompt\"].apply(lambda x: x.replace('\\n', ' ').split(\" \"))\n    df[\"total_prompt_words\"] = df[\"prompt\"].apply(lambda x: len(x.split(\" \")))\n    df[\"prompt_length\"] = df[\"prompt\"].apply(lambda x: len(x))\n\n    df[\"response_a_words\"] = df[\"response_a\"].apply(lambda x: x.replace('\\n', ' ').split(\" \"))\n    df[\"total_response_a_words\"] = df[\"response_a\"].apply(lambda x: len(x.split(\" \")))\n    df[\"response_a_length\"] = df[\"response_a\"].apply(lambda x: len(x))\n\n    df[\"response_b_words\"] = df[\"response_b\"].apply(lambda x: x.replace('\\n', ' ').split(\" \"))\n    df[\"total_response_b_words\"] = df[\"response_b\"].apply(lambda x: len(x.split(\" \")))\n    df[\"response_b_length\"] = df[\"response_b\"].apply(lambda x: len(x))\n    \n    return df\n\ndef truncate_text(df, column_name, max_length=512):\n    df[f\"{column_name}\"] = df[column_name].str[:max_length]\n    return df\n\ntrain = add_label(train)\ntrain = add_stats(train)\ntrain = truncate_text(train, 'prompt')\ntrain = truncate_text(train, 'response_a')\ntrain = truncate_text(train, 'response_b')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:30.499863Z","iopub.execute_input":"2024-05-05T18:08:30.500159Z","iopub.status.idle":"2024-05-05T18:08:36.302115Z","shell.execute_reply.started":"2024-05-05T18:08:30.500134Z","shell.execute_reply":"2024-05-05T18:08:36.301012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization\n- In this notebook, i will use `prompt`, `response_a`, `response_b`.","metadata":{}},{"cell_type":"code","source":"class Tokenize(object):\n    def __init__(self, train, valid):\n        self.tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n        self.train = train\n        self.valid = valid\n        \n    def get_dataset(self, df):\n        ds = Dataset.from_dict({\n                'id': [e for e in df['id']],\n                'prompt': [ft for ft in df['prompt']],\n                'response_a': [ft for ft in df['response_a']],\n                'response_b': [ft for ft in df['response_b']],\n                'label': [s for s in df['label']],\n            })\n        return ds\n    \n    def tokenize_function(self, df):\n        tokenized_inputs = self.tokenizer(\n            df['prompt'], df['response_a'], df['response_b'],\n            truncation=True, padding=True, max_length=CFG.max_length\n        )\n        return tokenized_inputs\n    \n    def __call__(self):\n        train_ds = self.get_dataset(train)\n        valid_ds = self.get_dataset(valid)\n        \n        tokenized_train = train_ds.map(\n            self.tokenize_function, batched=True\n        )\n        tokenized_valid = valid_ds.map(\n            self.tokenize_function, batched=True\n        )\n        \n        return tokenized_train, tokenized_valid, self.tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:36.303788Z","iopub.execute_input":"2024-05-05T18:08:36.304065Z","iopub.status.idle":"2024-05-05T18:08:36.314033Z","shell.execute_reply.started":"2024-05-05T18:08:36.304042Z","shell.execute_reply":"2024-05-05T18:08:36.313058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute metrics","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    score = log_loss(labels, predictions)\n    results = {\n        'score': score\n    }\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:36.315234Z","iopub.execute_input":"2024-05-05T18:08:36.315562Z","iopub.status.idle":"2024-05-05T18:08:36.349943Z","shell.execute_reply.started":"2024-05-05T18:08:36.315535Z","shell.execute_reply":"2024-05-05T18:08:36.348901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV split","metadata":{}},{"cell_type":"code","source":"data = train.copy()\ndata[\"label\"] = data[\"label\"].astype('int32')\nskf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\nfor i, (_, val_index) in enumerate(skf.split(data, data[\"label\"])):\n    data.loc[val_index, \"fold\"] = i\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:36.351197Z","iopub.execute_input":"2024-05-05T18:08:36.351633Z","iopub.status.idle":"2024-05-05T18:08:36.454785Z","shell.execute_reply.started":"2024-05-05T18:08:36.351603Z","shell.execute_reply":"2024-05-05T18:08:36.453783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.debug:\n    display(data.groupby('fold').size())\n    data = data.sample(n=1000, random_state=0).reset_index(drop=True)\n    display(data.groupby('fold').size())","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:36.456138Z","iopub.execute_input":"2024-05-05T18:08:36.456824Z","iopub.status.idle":"2024-05-05T18:08:36.475065Z","shell.execute_reply.started":"2024-05-05T18:08:36.456788Z","shell.execute_reply":"2024-05-05T18:08:36.474286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set Training Args","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    fp16=True,\n    learning_rate=CFG.lr,\n    per_device_train_batch_size=CFG.train_batch_size,\n    per_device_eval_batch_size=CFG.eval_batch_size,\n    num_train_epochs=CFG.train_epochs,\n    weight_decay=CFG.weight_decay,\n    evaluation_strategy='epoch',\n    metric_for_best_model='score',\n    save_strategy='epoch',\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    report_to='none',\n    warmup_ratio=CFG.warmup_ratio,\n    optim='adamw_torch'\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:36.476291Z","iopub.execute_input":"2024-05-05T18:08:36.476662Z","iopub.status.idle":"2024-05-05T18:08:36.587758Z","shell.execute_reply.started":"2024-05-05T18:08:36.476626Z","shell.execute_reply":"2024-05-05T18:08:36.586754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training by hold","metadata":{}},{"cell_type":"code","source":"for fold in range(len(data['fold'].unique())):\n    train = data[data['fold'] != fold]\n    valid = data[data['fold'] == fold]\n    \n    # ADD NEW TOKENS for (\"\\n\") new paragraph and (\" \"*2) double space\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n    tokenizer.add_tokens([AddedToken(\"\\n\", normalized=False)])\n    tokenizer.add_tokens([AddedToken(\" \"*2, normalized=False)])\n    tokenize = Tokenize(train, valid)\n    tokenized_train, tokenized_valid, tokenizer = tokenize()\n    \n    model = AutoModelForSequenceClassification.from_pretrained(CFG.model, num_labels=CFG.num_labels)\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_train,\n        eval_dataset=tokenized_valid,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    \n    trainer.train()\n    \n    y_true = valid['label'].values\n    predictions = trainer.predict(tokenized_valid).predictions\n    predictions = predictions.argmax(axis=1)# + 1\n    cm = confusion_matrix(y_true, predictions, labels=[x for x in range(0,3)])\n    draw_cm = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=[x for x in range(0,3)])\n    draw_cm.plot()\n    plt.show()\n    \n    trainer.save_model(f'deberta-v3-xsmall_fold_{fold}')\n    tokenizer.save_pretrained(f'deberta-v3-xsmall_fold_{fold}')\n    \n    valid.to_csv(f'valid_df_fold_{fold}.csv', index=False)\n    break # just fold 0 train for test","metadata":{"execution":{"iopub.status.busy":"2024-05-05T18:08:36.58895Z","iopub.execute_input":"2024-05-05T18:08:36.589301Z","iopub.status.idle":"2024-05-05T18:11:16.676988Z","shell.execute_reply.started":"2024-05-05T18:08:36.589271Z","shell.execute_reply":"2024-05-05T18:11:16.675992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n- https://www.kaggle.com/code/hashidoyuto/deberta-baseline-aes2-0-train\n- https://www.kaggle.com/code/cdeotte/deberta-v3-small-starter-cv-0-820-lb-0-800?scriptVersionId=174239814","metadata":{}}]}